import argparse
import json
import sys
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse

import pandas as pd


CATEGORY_ASTAR = "A*"
CATEGORY_RINC = "–†–ò–ù–¶"
CATEGORY_GREY = "–°–µ—Ä–∞—è –∑–æ–Ω–∞"
CATEGORY_PREPRINT = "–ü—Ä–µ–ø—Ä–∏–Ω—Ç"
CATEGORY_STANDARD = "–°—Ç–∞–Ω–¥–∞—Ä—Ç/–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è"
CATEGORY_UNKNOWN = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"


def read_json_file(file_path: str) -> Any:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError as e:
        raise SystemExit(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}") from e
    except json.JSONDecodeError as e:
        raise SystemExit(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON –≤ '{file_path}': {e}") from e


def load_bibliography(file_path: str) -> pd.DataFrame:
    data = read_json_file(file_path)
    if isinstance(data, dict):
        # Try common container keys
        for key in ("items", "records", "data", "entries"):
            if key in data and isinstance(data[key], list):
                data = data[key]
                break
    if not isinstance(data, list):
        raise SystemExit("–û–∂–∏–¥–∞–ª—Å—è JSON-–º–∞—Å—Å–∏–≤ –∑–∞–ø–∏—Å–µ–π –∏–ª–∏ –æ–±—ä–µ–∫—Ç —Å –º–∞—Å—Å–∏–≤–æ–º –ø–æ –∫–ª—é—á—É 'items/records'.")
    return pd.DataFrame(data)


def get_first_present(row: pd.Series, candidates: List[str]) -> str:
    for key in candidates:
        if key in row and pd.notna(row[key]):
            value = str(row[key]).strip()
            if value and value != "-":
                return value
    return ""


def extract_domain(url: str) -> str:
    try:
        parsed = urlparse(url)
        host = parsed.netloc.lower()
        return host.replace("www.", "")
    except Exception:
        return ""


def normalize_rank(rank_raw: str) -> str:
    """Back-compat simple normalization for single-value rank fields.

    Note: richer multi-scheme parsing is in parse_rank_schemes/collect_rank_text.
    """
    rank = (rank_raw or "").strip().lower()
    # unify common variants
    replacements = {
        "–∞*": "a*",  # Cyrillic 'a' to Latin
        "q-1": "q1",
        "q-2": "q2",
        "q-3": "q3",
        "q-4": "q4",
        "q 1": "q1",
        "q 2": "q2",
        "q 3": "q3",
        "q 4": "q4",
        "quartile 1": "q1",
        "quartile 2": "q2",
        "quartile 3": "q3",
        "quartile 4": "q4",
    }
    for src, dst in replacements.items():
        rank = rank.replace(src, dst)
    return rank


def get_values(row: pd.Series, keys: List[str]) -> List[str]:
    values: List[str] = []
    for key in keys:
        if key in row and pd.notna(row[key]):
            value = str(row[key]).strip()
            if value and value != "-":
                values.append(value)
    return values


def collect_rank_text(row: pd.Series) -> str:
    """Collect all known rank-related fields into a single text blob for parsing."""
    rank_fields = [
        # generic
        "journal_rank",
        "rank",
        "quartile",
        "scopus_quartile",
        "wos_quartile",
        "sjr_rank",
        "snip_rank",
        # business/management rankings
        "abdc_rank",
        "abdc",
        "core_rank",
        "core",
        "abs_rank",
        "ajg_rank",
        "cabs_rank",
        "ajg",
        "abs",
        "cabs",
    ]
    values = get_values(row, rank_fields)
    return " | ".join(values).lower()


def parse_rank_schemes(rank_text: str) -> Dict[str, bool]:
    """Parse rank text into boolean flags for multiple schemes.

    Returns a dict with keys like 'q1', 'q2', 'abdc_a', 'core_a*', 'abs_4*', etc.
    """
    text = (rank_text or "").lower()
    # normalize separators and dashes
    text = (
        text.replace("‚àí", "-")
        .replace("‚Äì", "-")
        .replace("‚Äî", "-")
        .replace(" ", "")
    )
    # cyrillic '–∞' to latin 'a'
    text = text.replace("–∞*", "a*")

    flags: Dict[str, bool] = {}

    def set_flag(name: str) -> None:
        flags[name] = True

    # SJR/Scopus/WoS quartiles
    for q in ("q1", "q2", "q3", "q4"):
        if q in text or f"quartile{q[-1]}" in text:
            set_flag(q)

    # ABDC
    if any(tok in text for tok in ("abdc",)):
        if "abdca*" in text or "a*abdc" in text:
            set_flag("abdc_a*")
        for grade in ("a*", "a", "b", "c"):
            if f"abdc{grade}" in text:
                set_flag(f"abdc_{grade}")
        # also patterns like 'abdc-a', 'abdc_a'
        for grade in ("a*", "a", "b", "c"):
            if f"abdc-{grade}" in text or f"abdc_{grade}" in text:
                set_flag(f"abdc_{grade}")

    # CORE
    if any(tok in text for tok in ("core")):
        for grade in ("a*", "a", "b", "c"):
            if f"core{grade}" in text or f"core-{grade}" in text or f"core_{grade}" in text:
                set_flag(f"core_{grade}")

    # ABS/AJG/CABS (CABS/ABS/AJG levels 4*,4,3,2,1)
    for scheme in ("abs", "ajg", "cabs"):
        if scheme in text:
            for level in ("4*", "4", "3", "2", "1"):
                lvl_norm = level.replace("*", "\u2605")  # prevent accidental partial matches
                if f"{scheme}{level}" in text or f"{scheme}-{level}" in text or f"{scheme}_{level}" in text:
                    set_flag(f"abs_{level}")
                # some datasets may contain unicode star ‚Äî normalize heuristically
                if f"{scheme}{lvl_norm}" in text:
                    set_flag(f"abs_{level}")

    return flags


def classify_row(row: pd.Series, treat_preprints_as_grey: bool = True) -> str:
    source = get_first_present(row, [
        "Source", "source", "Journal", "journal", "Publisher", "publisher", "Venue", "venue",
    ]).lower()
    url = get_first_present(row, ["URL", "url", "Link", "link", "Href", "href"])
    domain = extract_domain(url)
    # collect rank info across multiple possible fields
    rank_text = collect_rank_text(row)
    rank_flags = parse_rank_schemes(rank_text)

    text_blob = " ".join(filter(None, [source, domain, url])).lower()

    # RINC detection
    rinc_markers = (
        "rinc", "—Ä–∏–Ω—Ü", "elibrary", "e-library", "elibrary.ru", "e-library.ru", "cyberleninka", "cyberleninka.ru"
    )
    if any(marker in text_blob for marker in rinc_markers):
        return CATEGORY_RINC

    # Grey zone detection
    grey_markers = (
        "habr", "medium", "blog", "vc.ru", "substack", "teletype", "github.io", "dev.to", "t.me", "telegram",
        "researchgate"  # typically not peer-reviewed
    )

    # Preprint detection (optional as grey)
    preprint_markers = (
        "arxiv", "biorxiv", "bioarxiv", "medrxiv", "chemrxiv", "preprint"
    )

    if any(marker in text_blob for marker in grey_markers):
        return CATEGORY_GREY

    if any(marker in text_blob for marker in preprint_markers):
        return CATEGORY_GREY if treat_preprints_as_grey else CATEGORY_PREPRINT

    # Standards / Specifications (authoritative but not peer-reviewed)
    standard_domains = {
        "w3.org", "rfc-editor.org", "ietf.org", "iso.org", "ecma-international.org", "itu.int",
        "whatwg.org", "oasis-open.org", "khronos.org"
    }
    standard_markers = ("rfc", "recommendation", "specification", "technicalreport")
    if domain in standard_domains or any(m in text_blob for m in standard_markers):
        return CATEGORY_STANDARD

    # A* via quartiles/ranks across multiple schemes
    if (
        rank_flags.get("q1")
        or rank_flags.get("q2")
        or rank_flags.get("abdc_a*")
        or rank_flags.get("abdc_a")
        or rank_flags.get("core_a*")
        or rank_flags.get("core_a")
        or rank_flags.get("abs_4*")
        or rank_flags.get("abs_4")
    ):
        return CATEGORY_ASTAR

    return CATEGORY_UNKNOWN


def compute_stats(categories: pd.Series) -> Dict[str, float]:
    if categories.empty:
        return {}
    percentages = (categories.value_counts(normalize=True) * 100).round(2)
    return percentages.to_dict()


def print_report(stats: Dict[str, float], a_star_min: float, rinc_min: float, grey_max: float) -> None:
    print("\nüìä –û—Ç—á—ë—Ç –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
    if not stats:
        print("‚Äî –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—á—ë—Ç–∞")
    else:
        for cat, pct in sorted(stats.items(), key=lambda kv: (-kv[1], kv[0])):
            print(f"‚Äî {cat}: {pct:.2f}%")

    a_star_pct = stats.get(CATEGORY_ASTAR, 0.0)
    rinc_pct = stats.get(CATEGORY_RINC, 0.0)
    grey_pct = stats.get(CATEGORY_GREY, 0.0)

    print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –∫–∞—á–µ—Å—Ç–≤–∞:")
    print(f"‚â• {a_star_min:.0f}% ‚Äî {CATEGORY_ASTAR}: ", "OK" if a_star_pct >= a_star_min else "FAIL")
    print(f"‚â• {rinc_min:.0f}% ‚Äî {CATEGORY_RINC}: ", "OK" if rinc_pct >= rinc_min else "FAIL")
    print(f"‚â§ {grey_max:.0f}% ‚Äî {CATEGORY_GREY}: ", "OK" if grey_pct <= grey_max else "FAIL")


def write_excel(df: pd.DataFrame, stats: Dict[str, float], output_path: str) -> None:
    try:
        with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
            df.to_excel(writer, index=False, sheet_name="–ó–∞–ø–∏—Å–∏")
            summary_df = (
                pd.Series(stats, name="–ü—Ä–æ—Ü–µ–Ω—Ç")
                .rename_axis("–ö–∞—Ç–µ–≥–æ—Ä–∏—è")
                .reset_index()
            )
            summary_df.to_excel(writer, index=False, sheet_name="–°–≤–æ–¥–∫–∞")
        print(f"\nüíæ –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ '{output_path}'")
    except Exception as e:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å Excel ('{output_path}'): {e}")


def main(argv: Optional[List[str]] = None) -> int:
    parser = argparse.ArgumentParser(description="–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–ø–∏—Å–∫–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã (—á–∞—Å—Ç—å 3)")
    parser.add_argument("--input", required=True, help="–ü—É—Ç—å –∫ JSON-—Ñ–∞–π–ª—É —Å–æ —Å–ø–∏—Å–∫–æ–º –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã")
    parser.add_argument("--output", default="bibliography_report.xlsx", help="–ü—É—Ç—å –∫ Excel-–æ—Ç—á—ë—Ç—É (xlsx)")
    parser.add_argument("--a-star-min", type=float, default=50.0, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç A* (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 50)")
    parser.add_argument("--rinc-min", type=float, default=15.0, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç –†–ò–ù–¶ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 15)")
    parser.add_argument("--grey-max", type=float, default=10.0, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç –°–µ—Ä–æ–π –∑–æ–Ω—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10)")
    parser.add_argument("--treat-preprints-as-grey", action="store_true", help="–°—á–∏—Ç–∞—Ç—å arXiv/–ø—Ä–µ–ø—Ä–∏–Ω—Ç—ã —Å–µ—Ä–æ–π –∑–æ–Ω–æ–π")

    args = parser.parse_args(argv)

    df = load_bibliography(args.input)
    if df.empty:
        print("–í—Ö–æ–¥–Ω–æ–π —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç.")
        return 0

    df = df.copy()
    df["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"] = df.apply(lambda row: classify_row(row, treat_preprints_as_grey=args.treat_preprints_as_grey), axis=1)
    stats = compute_stats(df["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"]) or {}

    print_report(stats, a_star_min=args["a_star_min"] if isinstance(args, dict) else args.a_star_min,
                 rinc_min=args["rinc_min"] if isinstance(args, dict) else args.rinc_min,
                 grey_max=args["grey_max"] if isinstance(args, dict) else args.grey_max)

    if args.output:
        write_excel(df, stats, args.output)

    return 0


if __name__ == "__main__":
    sys.exit(main())
