import argparse
import json
import sys
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

import pandas as pd


CATEGORY_ASTAR = "A*"
CATEGORY_RINC = "–†–ò–ù–¶"
CATEGORY_GREY = "–°–µ—Ä–∞—è –∑–æ–Ω–∞"
CATEGORY_UNKNOWN = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"


def read_json_file(file_path: str) -> Any:
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError as e:
        raise SystemExit(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}") from e
    except json.JSONDecodeError as e:
        raise SystemExit(f"–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π JSON –≤ '{file_path}': {e}") from e


def load_bibliography(file_path: str) -> pd.DataFrame:
    data = read_json_file(file_path)
    if isinstance(data, dict):
        # Try common container keys
        for key in ("items", "records", "data", "entries"):
            if key in data and isinstance(data[key], list):
                data = data[key]
                break
    if not isinstance(data, list):
        raise SystemExit("–û–∂–∏–¥–∞–ª—Å—è JSON-–º–∞—Å—Å–∏–≤ –∑–∞–ø–∏—Å–µ–π –∏–ª–∏ –æ–±—ä–µ–∫—Ç —Å –º–∞—Å—Å–∏–≤–æ–º –ø–æ –∫–ª—é—á—É 'items/records'.")
    return pd.DataFrame(data)


def get_first_present(row: pd.Series, candidates: List[str]) -> str:
    for key in candidates:
        if key in row and pd.notna(row[key]):
            value = str(row[key]).strip()
            if value and value != "-":
                return value
    return ""


def extract_domain(url: str) -> str:
    try:
        parsed = urlparse(url)
        host = parsed.netloc.lower()
        return host.replace("www.", "")
    except Exception:
        return ""


def normalize_rank(rank_raw: str) -> str:
    rank = (rank_raw or "").strip().lower()
    # unify similar encodings
    replacements = {
        "–∞*": "a*",  # Cyrillic 'a' to Latin
        "q-1": "q1",
        "q-2": "q2",
        "q 1": "q1",
        "q 2": "q2",
    }
    for src, dst in replacements.items():
        rank = rank.replace(src, dst)
    return rank


def classify_row(row: pd.Series, treat_preprints_as_grey: bool = True) -> str:
    source = get_first_present(row, [
        "Source", "source", "Journal", "journal", "Publisher", "publisher", "Venue", "venue",
    ]).lower()
    url = get_first_present(row, ["URL", "url", "Link", "link", "Href", "href"])
    domain = extract_domain(url)
    rank = normalize_rank(get_first_present(row, [
        "journal_rank", "rank", "quartile", "scopus_quartile", "wos_quartile", "sjr_rank", "snip_rank"
    ]))

    text_blob = " ".join(filter(None, [source, domain, url])).lower()

    # RINC detection
    rinc_markers = (
        "rinc", "—Ä–∏–Ω—Ü", "elibrary", "e-library", "elibrary.ru", "e-library.ru", "cyberleninka", "cyberleninka.ru"
    )
    if any(marker in text_blob for marker in rinc_markers):
        return CATEGORY_RINC

    # Grey zone detection
    grey_markers = (
        "habr", "medium", "blog", "vc.ru", "substack", "teletype", "github.io", "dev.to", "t.me", "telegram",
        "researchgate"  # typically not peer-reviewed
    )

    # Preprint detection (optional as grey)
    preprint_markers = (
        "arxiv", "biorxiv", "bioarxiv", "medrxiv", "chemrxiv", "preprint"
    )

    if any(marker in text_blob for marker in grey_markers):
        return CATEGORY_GREY

    if treat_preprints_as_grey and any(marker in text_blob for marker in preprint_markers):
        return CATEGORY_GREY

    # A* via quartiles/ranks
    if rank in {"q1", "q2", "a*", "a"}:
        return CATEGORY_ASTAR

    return CATEGORY_UNKNOWN


def compute_stats(categories: pd.Series) -> Dict[str, float]:
    if categories.empty:
        return {}
    percentages = (categories.value_counts(normalize=True) * 100).round(2)
    return percentages.to_dict()


def print_report(stats: Dict[str, float], a_star_min: float, rinc_min: float, grey_max: float) -> None:
    print("\nüìä –û—Ç—á—ë—Ç –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
    if not stats:
        print("‚Äî –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç—á—ë—Ç–∞")
    else:
        for cat, pct in stats.items():
            print(f"‚Äî {cat}: {pct:.2f}%")

    a_star_pct = stats.get(CATEGORY_ASTAR, 0.0)
    rinc_pct = stats.get(CATEGORY_RINC, 0.0)
    grey_pct = stats.get(CATEGORY_GREY, 0.0)

    print("\n‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –∫–∞—á–µ—Å—Ç–≤–∞:")
    print(f"‚â• {a_star_min:.0f}% ‚Äî {CATEGORY_ASTAR}: ", "OK" if a_star_pct >= a_star_min else "FAIL")
    print(f"‚â• {rinc_min:.0f}% ‚Äî {CATEGORY_RINC}: ", "OK" if rinc_pct >= rinc_min else "FAIL")
    print(f"‚â§ {grey_max:.0f}% ‚Äî {CATEGORY_GREY}: ", "OK" if grey_pct <= grey_max else "FAIL")


def write_excel(df: pd.DataFrame, stats: Dict[str, float], output_path: str) -> None:
    try:
        with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
            df.to_excel(writer, index=False, sheet_name="–ó–∞–ø–∏—Å–∏")
            summary_df = (
                pd.Series(stats, name="–ü—Ä–æ—Ü–µ–Ω—Ç")
                .rename_axis("–ö–∞—Ç–µ–≥–æ—Ä–∏—è")
                .reset_index()
            )
            summary_df.to_excel(writer, index=False, sheet_name="–°–≤–æ–¥–∫–∞")
        print(f"\nüíæ –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ '{output_path}'")
    except Exception as e:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø–∏—Å–∞—Ç—å Excel ('{output_path}'): {e}")


def main(argv: Optional[List[str]] = None) -> int:
    parser = argparse.ArgumentParser(description="–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–ø–∏—Å–∫–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã (—á–∞—Å—Ç—å 3)")
    parser.add_argument("--input", required=True, help="–ü—É—Ç—å –∫ JSON-—Ñ–∞–π–ª—É —Å–æ —Å–ø–∏—Å–∫–æ–º –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã")
    parser.add_argument("--output", default="bibliography_report.xlsx", help="–ü—É—Ç—å –∫ Excel-–æ—Ç—á—ë—Ç—É (xlsx)")
    parser.add_argument("--a-star-min", type=float, default=50.0, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç A* (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 50)")
    parser.add_argument("--rinc-min", type=float, default=15.0, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç –†–ò–ù–¶ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 15)")
    parser.add_argument("--grey-max", type=float, default=10.0, help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ–Ω—Ç –°–µ—Ä–æ–π –∑–æ–Ω—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10)")
    parser.add_argument("--treat-preprints-as-grey", action="store_true", help="–°—á–∏—Ç–∞—Ç—å arXiv/–ø—Ä–µ–ø—Ä–∏–Ω—Ç—ã —Å–µ—Ä–æ–π –∑–æ–Ω–æ–π")

    args = parser.parse_args(argv)

    df = load_bibliography(args.input)
    if df.empty:
        print("–í—Ö–æ–¥–Ω–æ–π —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç.")
        return 0

    df = df.copy()
    df["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"] = df.apply(lambda row: classify_row(row, treat_preprints_as_grey=args.treat_preprints_as_grey), axis=1)
    stats = compute_stats(df["–ö–∞—Ç–µ–≥–æ—Ä–∏—è"]) or {}

    print_report(stats, a_star_min=args["a_star_min"] if isinstance(args, dict) else args.a_star_min,
                 rinc_min=args["rinc_min"] if isinstance(args, dict) else args.rinc_min,
                 grey_max=args["grey_max"] if isinstance(args, dict) else args.grey_max)

    if args.output:
        write_excel(df, stats, args.output)

    return 0


if __name__ == "__main__":
    sys.exit(main())
